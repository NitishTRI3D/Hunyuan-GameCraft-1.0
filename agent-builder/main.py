import argparse
import json
import os
import time
from datetime import datetime
import uuid

from tools.fal_klingai import generate_and_save
from tools.gemini_analyzer import analyze_video_motion, classify_frame_consistency
from tools.camera_planner import propose_movements, build_kling_prompt
from tools.frame_utils import extract_frames, extract_start_frame


def main() -> None:
    parser = argparse.ArgumentParser(
        description=(
            "Generate videos from an image using FAL Kling 2.1 Pro (Image-to-Video) in an agentic loop. "
            "Uses the same image for both head and tail frames."
        )
    )
    parser.add_argument(
        "--image",
        required=True,
        help="Path to the input image file",
    )
    parser.add_argument(
        "--description",
        required=True,
        help=(
            "Short description of the scene. Dynamic camera prompts will be built to explore the world."
        ),
    )
    parser.add_argument(
        "--duration",
        type=int,
        default=5,
        choices=[5, 10],
        help="Duration in seconds (5 or 10). Default: 5",
    )
    parser.add_argument("--run_id", default=None, help="Optional run id; autogenerated if not provided")
    parser.add_argument("--extract_frames_max", type=int, default=12, help="Max frames to extract per video")
    parser.add_argument("--num_videos", type=int, default=3, help="How many videos to generate in this run")
    parser.add_argument("--no_gemini", action="store_true", help="Disable Gemini call in analysis")

    args = parser.parse_args()

    inputs_dir = os.path.dirname(os.path.abspath(args.image))
    if not os.path.isfile(args.image):
        raise SystemExit(f"Input image not found: {args.image}")

    # Prepare run directories
    run_id = args.run_id or datetime.now().strftime("%Y%m%d_%H%M%S") + "_" + uuid.uuid4().hex[:6]
    base_out_dir = os.path.abspath(os.path.join("outputs", run_id))
    videos_dir = os.path.join(base_out_dir, "videos")
    frames_dir = os.path.join(base_out_dir, "frames")
    classified_dir = os.path.join(base_out_dir, "classified")
    good_dir = os.path.join(classified_dir, "good")
    bad_dir = os.path.join(classified_dir, "bad")
    for d in [videos_dir, frames_dir, good_dir, bad_dir]:
        os.makedirs(d, exist_ok=True)

    lineage = {
        "run_id": run_id,
        "started_at": datetime.now().isoformat(),
        "image": os.path.abspath(args.image),
        "description": args.description,
        "duration": args.duration,
        "items": [],
    }

    # Persist lineage incrementally for the UI
    lineage_path = os.path.join(base_out_dir, "lineage.json")
    def persist_lineage() -> None:
        with open(lineage_path, "w", encoding="utf-8") as f:
            json.dump(lineage, f, ensure_ascii=False, indent=2)
    persist_lineage()

    # Notes file setup
    notes_path = os.path.join(base_out_dir, "notes.txt")
    def note(msg: str) -> None:
        ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        with open(notes_path, "a", encoding="utf-8") as nf:
            nf.write(f"[{ts}] {msg}\n")

    movements = propose_movements(args.description, args.num_videos)
    note(f"Planned {len(movements)} camera movements for scene: {args.description}")
    for m in movements:
        mt = m.get("type")
        md = m.get("description")
        note(f"Plan: {mt} - {md}")

    for idx, move in enumerate(movements, start=1):
        kling_prompt = build_kling_prompt(args.description, move)
        base_name = os.path.splitext(os.path.basename(args.image))[0]
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        video_filename = f"kling_{base_name}_{args.duration}s_{idx:02d}_{ts}.mp4"
        output_path = os.path.join(videos_dir, video_filename)

        # Add placeholder lineage item so UI can render row immediately
        item = {
            "movement": move,
            "inputs": {
                "video_path": None,
                "kling_prompt": kling_prompt,
                "generation_seconds": None,
            },
            "video_analysis": None,
            "start_frame": None,
            "frames_dir": None,
            "frames": [],
            "status": "starting",
        }
        lineage["items"].append(item)
        persist_lineage()

        note(f"Starting video {idx}/{len(movements)} with movement '{move.get('type')}'. Output: {output_path}")
        t0 = time.monotonic()
        result = generate_and_save(
            image_path=args.image,
            description=args.description,
            output_path=output_path,
            duration_seconds=args.duration,
            prompt_override=kling_prompt,
        )
        t1 = time.monotonic()
        print(f"Saved video: {result['output_path']}")
        note(f"Saved video: {result['output_path']} (elapsed {(t1 - t0):.2f}s)")

        # Write a start-frame thumbnail immediately for UI
        video_stem = os.path.splitext(video_filename)[0]
        start_thumb_path = os.path.join(videos_dir, f"{video_stem}_start.jpg")
        if extract_start_frame(result['output_path'], start_thumb_path):
            note(f"Start frame extracted: {start_thumb_path}")
        else:
            note("Failed to extract start frame.")

        # Update lineage item after generation
        item["inputs"]["video_path"] = result["output_path"]
        item["inputs"]["generation_seconds"] = (t1 - t0)
        item["start_frame"] = start_thumb_path if os.path.exists(start_thumb_path) else None
        item["status"] = "generated"
        persist_lineage()

        note(f"Analyzing motion for video: {result['output_path']}")
        analysis = analyze_video_motion(
            video_path=result["output_path"],
            original_image_path=os.path.abspath(args.image),
            kling_prompt=str(result.get("prompt") or ""),
            description=args.description,
            generation_seconds=(t1 - t0),
            use_gemini=(not args.no_gemini),
        )
        analysis_path = os.path.splitext(result["output_path"])[0] + ".analysis.json"
        with open(analysis_path, "w", encoding="utf-8") as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        note(f"Motion analysis complete. has_motion={analysis.get('final',{}).get('has_motion')} saved: {analysis_path}")
        item["video_analysis"] = analysis_path
        persist_lineage()

        # If no motion detected, skip frame extraction/classification for this video
        final_has_motion = bool(analysis.get("final", {}).get("has_motion", False))
        if not final_has_motion:
            note("No motion detected; skipping frame extraction and classification for this video.")
            item["status"] = "no_motion"
            item["frames_dir"] = None
            item["frames"] = []
            persist_lineage()
            continue

        # Extract frames from this video
        video_frames_dir = os.path.join(frames_dir, os.path.splitext(video_filename)[0])
        os.makedirs(video_frames_dir, exist_ok=True)
        frame_paths = extract_frames(result["output_path"], video_frames_dir, max_frames=args.extract_frames_max)
        note(f"Extracted {len(frame_paths)} frames to {video_frames_dir}")
        item["frames_dir"] = video_frames_dir
        item["status"] = "classifying"
        persist_lineage()

        # Classify each frame and write per-frame JSON, then copy JSON to good/bad
        frame_entries = []
        good_count = 0
        bad_count = 0
        for fp in frame_paths:
            fr = classify_frame_consistency(
                original_image_path=os.path.abspath(args.image),
                frame_path=fp,
                original_description=args.description,
                movement=move,
                kling_prompt=str(result.get("prompt") or ""),
                source_video_path=result["output_path"],
                use_gemini=(not args.no_gemini),
            )
            # Save per-frame json alongside frame
            per_json = os.path.splitext(fp)[0] + ".json"
            with open(per_json, "w", encoding="utf-8") as f:
                json.dump(fr, f, ensure_ascii=False, indent=2)

            label = fr.get("classification", {}).get("label", "bad")
            target_dir = good_dir if label == "good" else bad_dir
            # Copy both image and json to classified bucket
            import shutil
            shutil.copy2(fp, os.path.join(target_dir, os.path.basename(fp)))
            shutil.copy2(per_json, os.path.join(target_dir, os.path.basename(per_json)))

            if label == "good":
                good_count += 1
            else:
                bad_count += 1
            note(f"Frame classified: {os.path.basename(fp)} -> {label}")

            entry = {
                "path": fp,
                "label": label,
                "json": per_json,
                "gemini_prompt": fr.get("gemini", {}).get("prompt"),
                "gemini_response": fr.get("gemini", {}).get("response_text"),
            }
            frame_entries.append(entry)
            # Persist each frame entry so UI updates progressively
            item["frames"].append(entry)
            persist_lineage()

        note(f"Video classification summary: good={good_count}, bad={bad_count}")
        item["status"] = "completed"
        if not item["frames"] and frame_entries:
            item["frames"] = frame_entries
        persist_lineage()

    persist_lineage()
    print(f"Run complete. Artifacts in: {base_out_dir}")
    note("Run complete. Lineage written.")


if __name__ == "__main__":
    main()


